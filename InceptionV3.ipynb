{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionV3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFoksCRiAktC",
        "colab_type": "code",
        "outputId": "8bf78089-7017-46f6-bb58-e65e08a39dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqEkn6qKIFc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling2D, Dense \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import TensorBoard, ReduceLROnPlateau, EarlyStopping, Callback\n",
        "from google.colab.patches import cv2_imshow\n",
        "import h5py\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32iUKot-ISSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = 6\n",
        "img_height = 150\n",
        "img_width = 150\n",
        "epochs_all_layers = 100\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5NtVGDNIZlI",
        "colab_type": "code",
        "outputId": "d48dc8cb-2cdf-4521-c1d8-99e8a261de4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "file_paths = [\"drive/My Drive/Data/Agricultural\",\"drive/My Drive/Data/Airport\",\"drive/My Drive/Data/Baseball\",\"drive/My Drive/Data/Beach\",\"drive/My Drive/Data/Bridge\",\"drive/My Drive/Data/Buildings\"]\n",
        "training_data = []\n",
        "label = -1\n",
        "for file_path in file_paths:\n",
        "  label = label + 1\n",
        "  for img in tqdm(os.listdir(file_path)):\n",
        "    path = os.path.join(file_path,img)\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.resize(image, (img_height, img_width))\n",
        "    training_data.append([np.array(image),np.array(label)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:47<00:00,  2.18it/s]\n",
            "100%|██████████| 100/100 [00:47<00:00,  2.19it/s]\n",
            "100%|██████████| 100/100 [00:46<00:00,  2.21it/s]\n",
            "100%|██████████| 100/100 [00:46<00:00,  2.19it/s]\n",
            "100%|██████████| 100/100 [00:46<00:00,  2.13it/s]\n",
            "100%|██████████| 100/100 [00:46<00:00,  2.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ5R3tdnJT82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = training_data\n",
        "dataset = np.array(dataset)\n",
        "features = dataset[:, 0]\n",
        "labels = dataset[:, 1]\n",
        "labels_one_hot_encoded = to_categorical(labels, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIKxhV_9JYDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature =[]\n",
        "for i in range(len(features)):\n",
        "    feature.append(features[i])\n",
        "\n",
        "features = np.asarray(feature)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnk7YbxJbU8",
        "colab_type": "code",
        "outputId": "8bcadb82-bc2a-4b8f-fe8b-551d7b712765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "cv2_imshow(x_train[200])\n",
        "print(y_train[200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAIAAACzY+a1AACUb0lEQVR4nGT9abRkyXEeCNri7vfe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\nqhkUIURCTDnXDmCDA2b2fwAPaFEeCRZlnQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=150x150 at 0x7F3F32C11F28>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhOk7lJTJcSA",
        "colab_type": "code",
        "outputId": "93293b5c-15b4-4cfc-911a-e9b5a08ba963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10642
        }
      },
      "source": [
        "model = InceptionV3(include_top = False,weights = 'imagenet',input_shape=(img_height,img_width,3))\n",
        "model.layers.pop()\n",
        "\n",
        "model.outputs = [model.layers[-2].output]\n",
        "\n",
        "model=Model(model.input,model.outputs)\n",
        "print(model.summary())\n",
        "newfeatures=model.predict(features)\n",
        "#features_test=model.predict(test_img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_659 (Conv2D)             (None, 74, 74, 32)   864         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_659 (BatchN (None, 74, 74, 32)   96          conv2d_659[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_659 (Activation)     (None, 74, 74, 32)   0           batch_normalization_659[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_660 (Conv2D)             (None, 72, 72, 32)   9216        activation_659[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_660 (BatchN (None, 72, 72, 32)   96          conv2d_660[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_660 (Activation)     (None, 72, 72, 32)   0           batch_normalization_660[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_661 (Conv2D)             (None, 72, 72, 64)   18432       activation_660[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_661 (BatchN (None, 72, 72, 64)   192         conv2d_661[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_661 (Activation)     (None, 72, 72, 64)   0           batch_normalization_661[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling2D) (None, 35, 35, 64)   0           activation_661[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_662 (Conv2D)             (None, 35, 35, 80)   5120        max_pooling2d_29[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_662 (BatchN (None, 35, 35, 80)   240         conv2d_662[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_662 (Activation)     (None, 35, 35, 80)   0           batch_normalization_662[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_663 (Conv2D)             (None, 33, 33, 192)  138240      activation_662[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_663 (BatchN (None, 33, 33, 192)  576         conv2d_663[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_663 (Activation)     (None, 33, 33, 192)  0           batch_normalization_663[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling2D) (None, 16, 16, 192)  0           activation_663[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_667 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_30[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_667 (BatchN (None, 16, 16, 64)   192         conv2d_667[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_667 (Activation)     (None, 16, 16, 64)   0           batch_normalization_667[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_665 (Conv2D)             (None, 16, 16, 48)   9216        max_pooling2d_30[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_668 (Conv2D)             (None, 16, 16, 96)   55296       activation_667[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_665 (BatchN (None, 16, 16, 48)   144         conv2d_665[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_668 (BatchN (None, 16, 16, 96)   288         conv2d_668[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_665 (Activation)     (None, 16, 16, 48)   0           batch_normalization_665[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_668 (Activation)     (None, 16, 16, 96)   0           batch_normalization_668[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_64 (AveragePo (None, 16, 16, 192)  0           max_pooling2d_30[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_664 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_30[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_666 (Conv2D)             (None, 16, 16, 64)   76800       activation_665[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_669 (Conv2D)             (None, 16, 16, 96)   82944       activation_668[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_670 (Conv2D)             (None, 16, 16, 32)   6144        average_pooling2d_64[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_664 (BatchN (None, 16, 16, 64)   192         conv2d_664[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_666 (BatchN (None, 16, 16, 64)   192         conv2d_666[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_669 (BatchN (None, 16, 16, 96)   288         conv2d_669[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_670 (BatchN (None, 16, 16, 32)   96          conv2d_670[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_664 (Activation)     (None, 16, 16, 64)   0           batch_normalization_664[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_666 (Activation)     (None, 16, 16, 64)   0           batch_normalization_666[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_669 (Activation)     (None, 16, 16, 96)   0           batch_normalization_669[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_670 (Activation)     (None, 16, 16, 32)   0           batch_normalization_670[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_664[0][0]             \n",
            "                                                                 activation_666[0][0]             \n",
            "                                                                 activation_669[0][0]             \n",
            "                                                                 activation_670[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_674 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_674 (BatchN (None, 16, 16, 64)   192         conv2d_674[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_674 (Activation)     (None, 16, 16, 64)   0           batch_normalization_674[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_672 (Conv2D)             (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_675 (Conv2D)             (None, 16, 16, 96)   55296       activation_674[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_672 (BatchN (None, 16, 16, 48)   144         conv2d_672[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_675 (BatchN (None, 16, 16, 96)   288         conv2d_675[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_672 (Activation)     (None, 16, 16, 48)   0           batch_normalization_672[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_675 (Activation)     (None, 16, 16, 96)   0           batch_normalization_675[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_65 (AveragePo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_671 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_673 (Conv2D)             (None, 16, 16, 64)   76800       activation_672[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_676 (Conv2D)             (None, 16, 16, 96)   82944       activation_675[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_677 (Conv2D)             (None, 16, 16, 64)   16384       average_pooling2d_65[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_671 (BatchN (None, 16, 16, 64)   192         conv2d_671[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_673 (BatchN (None, 16, 16, 64)   192         conv2d_673[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_676 (BatchN (None, 16, 16, 96)   288         conv2d_676[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_677 (BatchN (None, 16, 16, 64)   192         conv2d_677[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_671 (Activation)     (None, 16, 16, 64)   0           batch_normalization_671[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_673 (Activation)     (None, 16, 16, 64)   0           batch_normalization_673[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_676 (Activation)     (None, 16, 16, 96)   0           batch_normalization_676[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_677 (Activation)     (None, 16, 16, 64)   0           batch_normalization_677[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_671[0][0]             \n",
            "                                                                 activation_673[0][0]             \n",
            "                                                                 activation_676[0][0]             \n",
            "                                                                 activation_677[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_681 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_681 (BatchN (None, 16, 16, 64)   192         conv2d_681[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_681 (Activation)     (None, 16, 16, 64)   0           batch_normalization_681[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_679 (Conv2D)             (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_682 (Conv2D)             (None, 16, 16, 96)   55296       activation_681[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_679 (BatchN (None, 16, 16, 48)   144         conv2d_679[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_682 (BatchN (None, 16, 16, 96)   288         conv2d_682[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_679 (Activation)     (None, 16, 16, 48)   0           batch_normalization_679[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_682 (Activation)     (None, 16, 16, 96)   0           batch_normalization_682[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_66 (AveragePo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_678 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_680 (Conv2D)             (None, 16, 16, 64)   76800       activation_679[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_683 (Conv2D)             (None, 16, 16, 96)   82944       activation_682[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_684 (Conv2D)             (None, 16, 16, 64)   18432       average_pooling2d_66[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_678 (BatchN (None, 16, 16, 64)   192         conv2d_678[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_680 (BatchN (None, 16, 16, 64)   192         conv2d_680[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_683 (BatchN (None, 16, 16, 96)   288         conv2d_683[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_684 (BatchN (None, 16, 16, 64)   192         conv2d_684[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_678 (Activation)     (None, 16, 16, 64)   0           batch_normalization_678[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_680 (Activation)     (None, 16, 16, 64)   0           batch_normalization_680[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_683 (Activation)     (None, 16, 16, 96)   0           batch_normalization_683[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_684 (Activation)     (None, 16, 16, 64)   0           batch_normalization_684[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_678[0][0]             \n",
            "                                                                 activation_680[0][0]             \n",
            "                                                                 activation_683[0][0]             \n",
            "                                                                 activation_684[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_686 (Conv2D)             (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_686 (BatchN (None, 16, 16, 64)   192         conv2d_686[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_686 (Activation)     (None, 16, 16, 64)   0           batch_normalization_686[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_687 (Conv2D)             (None, 16, 16, 96)   55296       activation_686[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_687 (BatchN (None, 16, 16, 96)   288         conv2d_687[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_687 (Activation)     (None, 16, 16, 96)   0           batch_normalization_687[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_685 (Conv2D)             (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_688 (Conv2D)             (None, 7, 7, 96)     82944       activation_687[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_685 (BatchN (None, 7, 7, 384)    1152        conv2d_685[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_688 (BatchN (None, 7, 7, 96)     288         conv2d_688[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_685 (Activation)     (None, 7, 7, 384)    0           batch_normalization_685[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_688 (Activation)     (None, 7, 7, 96)     0           batch_normalization_688[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling2D) (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_685[0][0]             \n",
            "                                                                 activation_688[0][0]             \n",
            "                                                                 max_pooling2d_31[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_693 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_693 (BatchN (None, 7, 7, 128)    384         conv2d_693[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_693 (Activation)     (None, 7, 7, 128)    0           batch_normalization_693[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_694 (Conv2D)             (None, 7, 7, 128)    114688      activation_693[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_694 (BatchN (None, 7, 7, 128)    384         conv2d_694[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_694 (Activation)     (None, 7, 7, 128)    0           batch_normalization_694[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_690 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_695 (Conv2D)             (None, 7, 7, 128)    114688      activation_694[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_690 (BatchN (None, 7, 7, 128)    384         conv2d_690[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_695 (BatchN (None, 7, 7, 128)    384         conv2d_695[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_690 (Activation)     (None, 7, 7, 128)    0           batch_normalization_690[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_695 (Activation)     (None, 7, 7, 128)    0           batch_normalization_695[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_691 (Conv2D)             (None, 7, 7, 128)    114688      activation_690[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_696 (Conv2D)             (None, 7, 7, 128)    114688      activation_695[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_691 (BatchN (None, 7, 7, 128)    384         conv2d_691[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_696 (BatchN (None, 7, 7, 128)    384         conv2d_696[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_691 (Activation)     (None, 7, 7, 128)    0           batch_normalization_691[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_696 (Activation)     (None, 7, 7, 128)    0           batch_normalization_696[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_67 (AveragePo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_689 (Conv2D)             (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_692 (Conv2D)             (None, 7, 7, 192)    172032      activation_691[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_697 (Conv2D)             (None, 7, 7, 192)    172032      activation_696[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_698 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_67[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_689 (BatchN (None, 7, 7, 192)    576         conv2d_689[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_692 (BatchN (None, 7, 7, 192)    576         conv2d_692[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_697 (BatchN (None, 7, 7, 192)    576         conv2d_697[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_698 (BatchN (None, 7, 7, 192)    576         conv2d_698[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_689 (Activation)     (None, 7, 7, 192)    0           batch_normalization_689[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_692 (Activation)     (None, 7, 7, 192)    0           batch_normalization_692[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_697 (Activation)     (None, 7, 7, 192)    0           batch_normalization_697[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_698 (Activation)     (None, 7, 7, 192)    0           batch_normalization_698[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_689[0][0]             \n",
            "                                                                 activation_692[0][0]             \n",
            "                                                                 activation_697[0][0]             \n",
            "                                                                 activation_698[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_703 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_703 (BatchN (None, 7, 7, 160)    480         conv2d_703[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_703 (Activation)     (None, 7, 7, 160)    0           batch_normalization_703[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_704 (Conv2D)             (None, 7, 7, 160)    179200      activation_703[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_704 (BatchN (None, 7, 7, 160)    480         conv2d_704[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_704 (Activation)     (None, 7, 7, 160)    0           batch_normalization_704[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_700 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_705 (Conv2D)             (None, 7, 7, 160)    179200      activation_704[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_700 (BatchN (None, 7, 7, 160)    480         conv2d_700[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_705 (BatchN (None, 7, 7, 160)    480         conv2d_705[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_700 (Activation)     (None, 7, 7, 160)    0           batch_normalization_700[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_705 (Activation)     (None, 7, 7, 160)    0           batch_normalization_705[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_701 (Conv2D)             (None, 7, 7, 160)    179200      activation_700[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_706 (Conv2D)             (None, 7, 7, 160)    179200      activation_705[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_701 (BatchN (None, 7, 7, 160)    480         conv2d_701[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_706 (BatchN (None, 7, 7, 160)    480         conv2d_706[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_701 (Activation)     (None, 7, 7, 160)    0           batch_normalization_701[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_706 (Activation)     (None, 7, 7, 160)    0           batch_normalization_706[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_68 (AveragePo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_699 (Conv2D)             (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_702 (Conv2D)             (None, 7, 7, 192)    215040      activation_701[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_707 (Conv2D)             (None, 7, 7, 192)    215040      activation_706[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_708 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_68[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_699 (BatchN (None, 7, 7, 192)    576         conv2d_699[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_702 (BatchN (None, 7, 7, 192)    576         conv2d_702[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_707 (BatchN (None, 7, 7, 192)    576         conv2d_707[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_708 (BatchN (None, 7, 7, 192)    576         conv2d_708[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_699 (Activation)     (None, 7, 7, 192)    0           batch_normalization_699[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_702 (Activation)     (None, 7, 7, 192)    0           batch_normalization_702[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_707 (Activation)     (None, 7, 7, 192)    0           batch_normalization_707[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_708 (Activation)     (None, 7, 7, 192)    0           batch_normalization_708[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_699[0][0]             \n",
            "                                                                 activation_702[0][0]             \n",
            "                                                                 activation_707[0][0]             \n",
            "                                                                 activation_708[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_713 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_713 (BatchN (None, 7, 7, 160)    480         conv2d_713[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_713 (Activation)     (None, 7, 7, 160)    0           batch_normalization_713[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_714 (Conv2D)             (None, 7, 7, 160)    179200      activation_713[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_714 (BatchN (None, 7, 7, 160)    480         conv2d_714[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_714 (Activation)     (None, 7, 7, 160)    0           batch_normalization_714[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_710 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_715 (Conv2D)             (None, 7, 7, 160)    179200      activation_714[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_710 (BatchN (None, 7, 7, 160)    480         conv2d_710[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_715 (BatchN (None, 7, 7, 160)    480         conv2d_715[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_710 (Activation)     (None, 7, 7, 160)    0           batch_normalization_710[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_715 (Activation)     (None, 7, 7, 160)    0           batch_normalization_715[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_711 (Conv2D)             (None, 7, 7, 160)    179200      activation_710[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_716 (Conv2D)             (None, 7, 7, 160)    179200      activation_715[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_711 (BatchN (None, 7, 7, 160)    480         conv2d_711[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_716 (BatchN (None, 7, 7, 160)    480         conv2d_716[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_711 (Activation)     (None, 7, 7, 160)    0           batch_normalization_711[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_716 (Activation)     (None, 7, 7, 160)    0           batch_normalization_716[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_69 (AveragePo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_709 (Conv2D)             (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_712 (Conv2D)             (None, 7, 7, 192)    215040      activation_711[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_717 (Conv2D)             (None, 7, 7, 192)    215040      activation_716[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_718 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_69[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_709 (BatchN (None, 7, 7, 192)    576         conv2d_709[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_712 (BatchN (None, 7, 7, 192)    576         conv2d_712[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_717 (BatchN (None, 7, 7, 192)    576         conv2d_717[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_718 (BatchN (None, 7, 7, 192)    576         conv2d_718[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_709 (Activation)     (None, 7, 7, 192)    0           batch_normalization_709[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_712 (Activation)     (None, 7, 7, 192)    0           batch_normalization_712[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_717 (Activation)     (None, 7, 7, 192)    0           batch_normalization_717[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_718 (Activation)     (None, 7, 7, 192)    0           batch_normalization_718[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_709[0][0]             \n",
            "                                                                 activation_712[0][0]             \n",
            "                                                                 activation_717[0][0]             \n",
            "                                                                 activation_718[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_723 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_723 (BatchN (None, 7, 7, 192)    576         conv2d_723[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_723 (Activation)     (None, 7, 7, 192)    0           batch_normalization_723[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_724 (Conv2D)             (None, 7, 7, 192)    258048      activation_723[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_724 (BatchN (None, 7, 7, 192)    576         conv2d_724[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_724 (Activation)     (None, 7, 7, 192)    0           batch_normalization_724[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_720 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_725 (Conv2D)             (None, 7, 7, 192)    258048      activation_724[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_720 (BatchN (None, 7, 7, 192)    576         conv2d_720[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_725 (BatchN (None, 7, 7, 192)    576         conv2d_725[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_720 (Activation)     (None, 7, 7, 192)    0           batch_normalization_720[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_725 (Activation)     (None, 7, 7, 192)    0           batch_normalization_725[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_721 (Conv2D)             (None, 7, 7, 192)    258048      activation_720[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_726 (Conv2D)             (None, 7, 7, 192)    258048      activation_725[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_721 (BatchN (None, 7, 7, 192)    576         conv2d_721[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_726 (BatchN (None, 7, 7, 192)    576         conv2d_726[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_721 (Activation)     (None, 7, 7, 192)    0           batch_normalization_721[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_726 (Activation)     (None, 7, 7, 192)    0           batch_normalization_726[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_70 (AveragePo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_719 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_722 (Conv2D)             (None, 7, 7, 192)    258048      activation_721[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_727 (Conv2D)             (None, 7, 7, 192)    258048      activation_726[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_728 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_70[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_719 (BatchN (None, 7, 7, 192)    576         conv2d_719[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_722 (BatchN (None, 7, 7, 192)    576         conv2d_722[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_727 (BatchN (None, 7, 7, 192)    576         conv2d_727[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_728 (BatchN (None, 7, 7, 192)    576         conv2d_728[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_719 (Activation)     (None, 7, 7, 192)    0           batch_normalization_719[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_722 (Activation)     (None, 7, 7, 192)    0           batch_normalization_722[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_727 (Activation)     (None, 7, 7, 192)    0           batch_normalization_727[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_728 (Activation)     (None, 7, 7, 192)    0           batch_normalization_728[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_719[0][0]             \n",
            "                                                                 activation_722[0][0]             \n",
            "                                                                 activation_727[0][0]             \n",
            "                                                                 activation_728[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_731 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_731 (BatchN (None, 7, 7, 192)    576         conv2d_731[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_731 (Activation)     (None, 7, 7, 192)    0           batch_normalization_731[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_732 (Conv2D)             (None, 7, 7, 192)    258048      activation_731[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_732 (BatchN (None, 7, 7, 192)    576         conv2d_732[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_732 (Activation)     (None, 7, 7, 192)    0           batch_normalization_732[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_729 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_733 (Conv2D)             (None, 7, 7, 192)    258048      activation_732[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_729 (BatchN (None, 7, 7, 192)    576         conv2d_729[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_733 (BatchN (None, 7, 7, 192)    576         conv2d_733[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_729 (Activation)     (None, 7, 7, 192)    0           batch_normalization_729[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_733 (Activation)     (None, 7, 7, 192)    0           batch_normalization_733[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_730 (Conv2D)             (None, 3, 3, 320)    552960      activation_729[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_734 (Conv2D)             (None, 3, 3, 192)    331776      activation_733[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_730 (BatchN (None, 3, 3, 320)    960         conv2d_730[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_734 (BatchN (None, 3, 3, 192)    576         conv2d_734[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_730 (Activation)     (None, 3, 3, 320)    0           batch_normalization_730[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_734 (Activation)     (None, 3, 3, 192)    0           batch_normalization_734[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling2D) (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_730[0][0]             \n",
            "                                                                 activation_734[0][0]             \n",
            "                                                                 max_pooling2d_32[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_739 (Conv2D)             (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_739 (BatchN (None, 3, 3, 448)    1344        conv2d_739[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_739 (Activation)     (None, 3, 3, 448)    0           batch_normalization_739[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_736 (Conv2D)             (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_740 (Conv2D)             (None, 3, 3, 384)    1548288     activation_739[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_736 (BatchN (None, 3, 3, 384)    1152        conv2d_736[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_740 (BatchN (None, 3, 3, 384)    1152        conv2d_740[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_736 (Activation)     (None, 3, 3, 384)    0           batch_normalization_736[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_740 (Activation)     (None, 3, 3, 384)    0           batch_normalization_740[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_737 (Conv2D)             (None, 3, 3, 384)    442368      activation_736[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_738 (Conv2D)             (None, 3, 3, 384)    442368      activation_736[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_741 (Conv2D)             (None, 3, 3, 384)    442368      activation_740[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_742 (Conv2D)             (None, 3, 3, 384)    442368      activation_740[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_71 (AveragePo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_735 (Conv2D)             (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_737 (BatchN (None, 3, 3, 384)    1152        conv2d_737[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_738 (BatchN (None, 3, 3, 384)    1152        conv2d_738[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_741 (BatchN (None, 3, 3, 384)    1152        conv2d_741[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_742 (BatchN (None, 3, 3, 384)    1152        conv2d_742[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_743 (Conv2D)             (None, 3, 3, 192)    245760      average_pooling2d_71[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_735 (BatchN (None, 3, 3, 320)    960         conv2d_735[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_737 (Activation)     (None, 3, 3, 384)    0           batch_normalization_737[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_738 (Activation)     (None, 3, 3, 384)    0           batch_normalization_738[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_741 (Activation)     (None, 3, 3, 384)    0           batch_normalization_741[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_742 (Activation)     (None, 3, 3, 384)    0           batch_normalization_742[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_743 (BatchN (None, 3, 3, 192)    576         conv2d_743[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_735 (Activation)     (None, 3, 3, 320)    0           batch_normalization_735[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_737[0][0]             \n",
            "                                                                 activation_738[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 3, 3, 768)    0           activation_741[0][0]             \n",
            "                                                                 activation_742[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_743 (Activation)     (None, 3, 3, 192)    0           batch_normalization_743[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_735[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_15[0][0]             \n",
            "                                                                 activation_743[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_748 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_748 (BatchN (None, 3, 3, 448)    1344        conv2d_748[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_748 (Activation)     (None, 3, 3, 448)    0           batch_normalization_748[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_749 (Conv2D)             (None, 3, 3, 384)    1548288     activation_748[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_749 (BatchN (None, 3, 3, 384)    1152        conv2d_749[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_749 (Activation)     (None, 3, 3, 384)    0           batch_normalization_749[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_750 (Conv2D)             (None, 3, 3, 384)    442368      activation_749[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_751 (Conv2D)             (None, 3, 3, 384)    442368      activation_749[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_750 (BatchN (None, 3, 3, 384)    1152        conv2d_750[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_751 (BatchN (None, 3, 3, 384)    1152        conv2d_751[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_750 (Activation)     (None, 3, 3, 384)    0           batch_normalization_750[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_751 (Activation)     (None, 3, 3, 384)    0           batch_normalization_751[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 3, 3, 768)    0           activation_750[0][0]             \n",
            "                                                                 activation_751[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 19,078,048\n",
            "Trainable params: 19,046,944\n",
            "Non-trainable params: 31,104\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UVS48sfLGnh",
        "colab_type": "code",
        "outputId": "c67ac927-c4eb-4084-bb4b-f672ef4928d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(newfeatures.shape)\n",
        "print(features.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600, 3, 3, 768)\n",
            "(600, 150, 150, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5W9stO9VHgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, X_test, y_train, Y_test= train_test_split(newfeatures, labels_one_hot_encoded, test_size = 0.30, stratify = labels_one_hot_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrNWGrXeVIub",
        "colab_type": "code",
        "outputId": "c5262b3f-42eb-4e2c-a374-907bd706d4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x1=x_train[:140,:,:,:]\n",
        "x2=x_train[140:280,:,:,:]\n",
        "x3=x_train[280:420,:,:,:]\n",
        "print(x1.shape)\n",
        "print(x2.shape)\n",
        "print(x3.shape)\n",
        "y1=y_train[:140,:]\n",
        "y2=y_train[140:280,:]\n",
        "y3=y_train[280:420,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(140, 3, 3, 768)\n",
            "(140, 3, 3, 768)\n",
            "(140, 3, 3, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQhEwvCNWqb5",
        "colab_type": "code",
        "outputId": "91ce09ce-46d6-47d4-f40d-a9702d4c2c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15490
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model,Sequential,load_model\n",
        "from keras.layers.core import Dense, Dropout, Activation,Layer\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import style\n",
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "\n",
        "#Over first dataset\n",
        "X_train=x1\n",
        "Y_train=y1\n",
        "X_train = X_train.reshape(-1,6912)\n",
        "X_test = X_test.reshape(-1,(6912))\n",
        "X_train = X_train.astype(\"float64\")\n",
        "X_test = X_test.astype(\"float64\")\n",
        "print(X_train.shape)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "input_img = Input(shape=(6912,))\n",
        "# encoder 1\n",
        "enc1 = Dense(256,activation='relu')(input_img)\n",
        "# output with softmax prob.\n",
        "decode1 = Dense(6912,activation='sigmoid')(enc1)\n",
        "encoder1 = Model(input_img,enc1)\n",
        "auto_enc1 = Model(input_img,decode1)\n",
        "auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "auto_enc1.fit(X_train,X_train,epochs=50,batch_size=128)\n",
        "\n",
        "encoder1.save('Encoder1')\n",
        "input1 = encoder1.predict(X_train)\n",
        "\n",
        "\n",
        "input_img = Input(shape=(input1.shape[1],))\n",
        "# encoder 2\n",
        "enc1 = Dense(128,activation='relu')(input_img)\n",
        "# output with softmax prob.\n",
        "decode1 = Dense(input1.shape[1],activation='sigmoid')(enc1)\n",
        "encoder2 = Model(input_img,enc1)\n",
        "auto_enc2 = Model(input_img,decode1)\n",
        "auto_enc2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "auto_enc2.fit(input1,input1,epochs=50,batch_size=128)\n",
        "\n",
        "encoder2.save('Encoder2')\n",
        "input2 = encoder2.predict(input1)\n",
        "\n",
        "# this is the combined model with a softmax layer to fine tune the weights.\n",
        "final1 = Sequential()\n",
        "x = load_model('Encoder1')\n",
        "final1.add(x)\n",
        "x = load_model('Encoder2')\n",
        "final1.add(x)\n",
        "final1.add(Dense(6,activation='softmax'))\n",
        "final1.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  \n",
        "final1.fit(X_train, Y_train, epochs=50,batch_size=128,validation_data = (X_test,Y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Over second dataset\n",
        "X_train=x2\n",
        "Y_train=y2\n",
        "X_train = X_train.reshape(-1,6912)\n",
        "X_test = X_test.reshape(-1,(6912))\n",
        "X_train = X_train.astype(\"float64\")\n",
        "X_test = X_test.astype(\"float64\")\n",
        "print(X_train.shape)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "input_img = Input(shape=(6912,))\n",
        "# encoder 1\n",
        "enc1 = Dense(256,activation='relu')(input_img)\n",
        "# output with softmax prob.\n",
        "decode1 = Dense(6912,activation='sigmoid')(enc1)\n",
        "encoder1 = Model(input_img,enc1)\n",
        "auto_enc1 = Model(input_img,decode1)\n",
        "auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "auto_enc1.fit(X_train,X_train,epochs=50,batch_size=128)\n",
        "\n",
        "encoder1.save('Encoder1')\n",
        "input1 = encoder1.predict(X_train)\n",
        "\n",
        "\n",
        "input_img = Input(shape=(input1.shape[1],))\n",
        "# encoder 2\n",
        "enc1 = Dense(128,activation='relu')(input_img)\n",
        "# output with softmax prob.\n",
        "decode1 = Dense(input1.shape[1],activation='sigmoid')(enc1)\n",
        "encoder2 = Model(input_img,enc1)\n",
        "auto_enc2 = Model(input_img,decode1)\n",
        "auto_enc2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "auto_enc2.fit(input1,input1,epochs=50,batch_size=128)\n",
        "\n",
        "encoder2.save('Encoder2')\n",
        "input2 = encoder2.predict(input1)\n",
        "\n",
        "# this is the combined model with a softmax layer to fine tune the weights.\n",
        "final2 = Sequential()\n",
        "x = load_model('Encoder1')\n",
        "\n",
        "final2.add(x)\n",
        "x = load_model('Encoder2')\n",
        "final2.add(x)\n",
        "final2.add(Dense(6,activation='softmax'))\n",
        "\n",
        "final2.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  \n",
        "final2.fit(X_train, Y_train, epochs=50,batch_size=128,validation_data = (X_test,Y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Over 3rd part\n",
        "X_train=x3\n",
        "Y_train=y3\n",
        "X_train = X_train.reshape(-1,6912)\n",
        "X_test = X_test.reshape(-1,(6912))\n",
        "X_train = X_train.astype(\"float64\")\n",
        "X_test = X_test.astype(\"float64\")\n",
        "print(X_train.shape)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "input_img = Input(shape=(6912,))\n",
        "# encoder 1\n",
        "enc1 = Dense(256,activation='relu')(input_img)\n",
        "# output with softmax prob.\n",
        "decode1 = Dense(6912,activation='sigmoid')(enc1)\n",
        "encoder1 = Model(input_img,enc1)\n",
        "auto_enc1 = Model(input_img,decode1)\n",
        "auto_enc1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "auto_enc1.fit(X_train,X_train,epochs=50,batch_size=128)\n",
        "\n",
        "encoder1.save('Encoder1')\n",
        "input1 = encoder1.predict(X_train)\n",
        "\n",
        "\n",
        "input_img = Input(shape=(input1.shape[1],))\n",
        "# encoder 2\n",
        "enc1 = Dense(128,activation='relu')(input_img)\n",
        "# output with softmax prob.\n",
        "decode1 = Dense(input1.shape[1],activation='sigmoid')(enc1)\n",
        "encoder2 = Model(input_img,enc1)\n",
        "auto_enc2 = Model(input_img,decode1)\n",
        "auto_enc2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "auto_enc2.fit(input1,input1,epochs=50,batch_size=128)\n",
        "\n",
        "encoder2.save('Encoder2')\n",
        "input2 = encoder2.predict(input1)\n",
        "\n",
        "# this is the combined model with a softmax layer to fine tune the weights.\n",
        "final3 = Sequential()\n",
        "x = load_model('Encoder1')\n",
        "\n",
        "final3.add(x)\n",
        "x = load_model('Encoder2')\n",
        "final3.add(x)\n",
        "final3.add(Dense(6,activation='softmax'))\n",
        "\n",
        "final3.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  \n",
        "final3.fit(X_train, Y_train, epochs=50,batch_size=128,validation_data = (X_test,Y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(140, 6912)\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 23s 163ms/step - loss: 0.6928 - acc: 0.1419\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 302us/step - loss: 0.6765 - acc: 0.2178\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 268us/step - loss: 0.6119 - acc: 0.2656\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 268us/step - loss: 0.4939 - acc: 0.2764\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 249us/step - loss: 0.3620 - acc: 0.2771\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 255us/step - loss: 0.2674 - acc: 0.2771\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.2300 - acc: 0.2771\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 253us/step - loss: 0.2294 - acc: 0.2771\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 253us/step - loss: 0.2372 - acc: 0.2771\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 255us/step - loss: 0.2418 - acc: 0.2771\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 253us/step - loss: 0.2398 - acc: 0.2771\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 277us/step - loss: 0.2332 - acc: 0.2771\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 275us/step - loss: 0.2247 - acc: 0.2771\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.2172 - acc: 0.2771\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 269us/step - loss: 0.2111 - acc: 0.2771\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 296us/step - loss: 0.2065 - acc: 0.2771\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 0.2036 - acc: 0.2771\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 291us/step - loss: 0.2007 - acc: 0.2771\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 311us/step - loss: 0.1987 - acc: 0.2771\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 271us/step - loss: 0.1971 - acc: 0.2771\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 256us/step - loss: 0.1948 - acc: 0.2771\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 262us/step - loss: 0.1923 - acc: 0.2771\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 253us/step - loss: 0.1901 - acc: 0.2771\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 256us/step - loss: 0.1878 - acc: 0.2771\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 253us/step - loss: 0.1855 - acc: 0.2771\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 269us/step - loss: 0.1832 - acc: 0.2771\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 260us/step - loss: 0.1809 - acc: 0.2771\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 262us/step - loss: 0.1786 - acc: 0.2771\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 273us/step - loss: 0.1763 - acc: 0.2771\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 268us/step - loss: 0.1743 - acc: 0.2771\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 266us/step - loss: 0.1719 - acc: 0.2771\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 286us/step - loss: 0.1694 - acc: 0.2771\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 286us/step - loss: 0.1671 - acc: 0.2771\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 284us/step - loss: 0.1651 - acc: 0.2771\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 272us/step - loss: 0.1634 - acc: 0.2771\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 276us/step - loss: 0.1620 - acc: 0.2771\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 276us/step - loss: 0.1611 - acc: 0.2771\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 273us/step - loss: 0.1601 - acc: 0.2771\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 297us/step - loss: 0.1592 - acc: 0.2771\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 258us/step - loss: 0.1584 - acc: 0.2771\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 295us/step - loss: 0.1580 - acc: 0.2771\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 297us/step - loss: 0.1580 - acc: 0.2771\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 273us/step - loss: 0.1577 - acc: 0.2771\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 269us/step - loss: 0.1568 - acc: 0.2771\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 332us/step - loss: 0.1560 - acc: 0.2771\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 0.1557 - acc: 0.2771\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 265us/step - loss: 0.1551 - acc: 0.2771\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 273us/step - loss: 0.1547 - acc: 0.2771\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 269us/step - loss: 0.1541 - acc: 0.2771\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 259us/step - loss: 0.1542 - acc: 0.2771\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 23s 163ms/step - loss: 0.8245 - acc: 0.1874\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 186us/step - loss: 0.4570 - acc: 0.2253\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 144us/step - loss: 0.1169 - acc: 0.2689\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 128us/step - loss: -0.2553 - acc: 0.3014\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 138us/step - loss: -0.6837 - acc: 0.3233\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 139us/step - loss: -1.1717 - acc: 0.3363\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 140us/step - loss: -1.7477 - acc: 0.3554\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 140us/step - loss: -2.3823 - acc: 0.3616\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 130us/step - loss: -3.1109 - acc: 0.3635\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 147us/step - loss: -3.9020 - acc: 0.3673\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 146us/step - loss: -4.7457 - acc: 0.3750\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 138us/step - loss: -5.5594 - acc: 0.3763\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 140us/step - loss: -6.2939 - acc: 0.3767\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 121us/step - loss: -6.8692 - acc: 0.3763\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 144us/step - loss: -7.2711 - acc: 0.3748\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 179us/step - loss: -7.5039 - acc: 0.3734\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 166us/step - loss: -7.6228 - acc: 0.3738\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 161us/step - loss: -7.6760 - acc: 0.3751\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 172us/step - loss: -7.6972 - acc: 0.3763\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 206us/step - loss: -7.7056 - acc: 0.3774\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 183us/step - loss: -7.7096 - acc: 0.3782\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 153us/step - loss: -7.7124 - acc: 0.3787\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 141us/step - loss: -7.7144 - acc: 0.3772\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 190us/step - loss: -7.7150 - acc: 0.3765\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 208us/step - loss: -7.7157 - acc: 0.3755\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 161us/step - loss: -7.7167 - acc: 0.3756\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 159us/step - loss: -7.7177 - acc: 0.3773\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 164us/step - loss: -7.7184 - acc: 0.3776\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 182us/step - loss: -7.7193 - acc: 0.3780\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 144us/step - loss: -7.7198 - acc: 0.3773\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 149us/step - loss: -7.7204 - acc: 0.3768\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 168us/step - loss: -7.7216 - acc: 0.3782\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 168us/step - loss: -7.7226 - acc: 0.3799\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 173us/step - loss: -7.7240 - acc: 0.3792\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 183us/step - loss: -7.7253 - acc: 0.3781\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 227us/step - loss: -7.7260 - acc: 0.3770\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 177us/step - loss: -7.7271 - acc: 0.3770\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 136us/step - loss: -7.7289 - acc: 0.3780\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 172us/step - loss: -7.7304 - acc: 0.3790\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 153us/step - loss: -7.7304 - acc: 0.3806\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 146us/step - loss: -7.7306 - acc: 0.3822\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 162us/step - loss: -7.7325 - acc: 0.3822\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 177us/step - loss: -7.7351 - acc: 0.3811\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 144us/step - loss: -7.7357 - acc: 0.3782\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 157us/step - loss: -7.7371 - acc: 0.3773\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 186us/step - loss: -7.7395 - acc: 0.3791\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 160us/step - loss: -7.7401 - acc: 0.3810\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 157us/step - loss: -7.7402 - acc: 0.3821\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 169us/step - loss: -7.7415 - acc: 0.3826\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 145us/step - loss: -7.7441 - acc: 0.3828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 140 samples, validate on 180 samples\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 25s 175ms/step - loss: 2.8627 - acc: 0.7238 - val_loss: 0.4664 - val_acc: 0.8333\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 364us/step - loss: 2.0257 - acc: 0.7190 - val_loss: 0.4648 - val_acc: 0.8333\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 293us/step - loss: 1.3359 - acc: 0.7750 - val_loss: 0.4636 - val_acc: 0.8333\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 1.2736 - acc: 0.7417 - val_loss: 0.4630 - val_acc: 0.8333\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 1.2591 - acc: 0.7298 - val_loss: 0.4630 - val_acc: 0.8333\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 289us/step - loss: 1.0327 - acc: 0.7774 - val_loss: 0.4632 - val_acc: 0.8333\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 339us/step - loss: 0.9322 - acc: 0.7964 - val_loss: 0.4633 - val_acc: 0.8333\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 321us/step - loss: 0.8996 - acc: 0.7964 - val_loss: 0.4628 - val_acc: 0.8333\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.8264 - acc: 0.8310 - val_loss: 0.4622 - val_acc: 0.8333\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 274us/step - loss: 0.8371 - acc: 0.8143 - val_loss: 0.4615 - val_acc: 0.8333\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.8454 - acc: 0.8012 - val_loss: 0.4608 - val_acc: 0.8333\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 275us/step - loss: 0.7974 - acc: 0.8345 - val_loss: 0.4600 - val_acc: 0.8333\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.7513 - acc: 0.8524 - val_loss: 0.4594 - val_acc: 0.8333\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 299us/step - loss: 0.7176 - acc: 0.8440 - val_loss: 0.4586 - val_acc: 0.8333\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.6881 - acc: 0.8357 - val_loss: 0.4578 - val_acc: 0.8333\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 287us/step - loss: 0.6456 - acc: 0.8369 - val_loss: 0.4569 - val_acc: 0.8333\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 277us/step - loss: 0.5781 - acc: 0.8476 - val_loss: 0.4560 - val_acc: 0.8333\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 302us/step - loss: 0.5032 - acc: 0.8679 - val_loss: 0.4552 - val_acc: 0.8333\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 299us/step - loss: 0.4465 - acc: 0.8631 - val_loss: 0.4544 - val_acc: 0.8333\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 291us/step - loss: 0.3934 - acc: 0.8583 - val_loss: 0.4536 - val_acc: 0.8333\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 292us/step - loss: 0.3476 - acc: 0.8655 - val_loss: 0.4529 - val_acc: 0.8333\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 284us/step - loss: 0.3207 - acc: 0.8607 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 0.3234 - acc: 0.8643 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 291us/step - loss: 0.3257 - acc: 0.8548 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.3082 - acc: 0.8607 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 287us/step - loss: 0.2930 - acc: 0.8690 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 294us/step - loss: 0.2900 - acc: 0.8821 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 273us/step - loss: 0.2944 - acc: 0.8690 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 297us/step - loss: 0.2931 - acc: 0.8679 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.2873 - acc: 0.8774 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 310us/step - loss: 0.2847 - acc: 0.8750 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 337us/step - loss: 0.2766 - acc: 0.8881 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 301us/step - loss: 0.2673 - acc: 0.8833 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 286us/step - loss: 0.2704 - acc: 0.8833 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.2729 - acc: 0.8774 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 292us/step - loss: 0.2659 - acc: 0.8821 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.2543 - acc: 0.8964 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 314us/step - loss: 0.2471 - acc: 0.9012 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 284us/step - loss: 0.2520 - acc: 0.8857 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 284us/step - loss: 0.2525 - acc: 0.8905 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 0.2453 - acc: 0.9012 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 294us/step - loss: 0.2484 - acc: 0.8881 - val_loss: 0.4518 - val_acc: 0.8333\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 288us/step - loss: 0.2548 - acc: 0.8774 - val_loss: 0.4517 - val_acc: 0.8333\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 284us/step - loss: 0.2409 - acc: 0.8893 - val_loss: 0.4516 - val_acc: 0.8333\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.2320 - acc: 0.9083 - val_loss: 0.4516 - val_acc: 0.8333\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 276us/step - loss: 0.2447 - acc: 0.8964 - val_loss: 0.4516 - val_acc: 0.8333\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.2469 - acc: 0.8905 - val_loss: 0.4518 - val_acc: 0.8333\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 289us/step - loss: 0.2343 - acc: 0.9024 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 294us/step - loss: 0.2237 - acc: 0.9131 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.2227 - acc: 0.9155 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "(140, 6912)\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 24s 170ms/step - loss: 0.6929 - acc: 0.1445\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 336us/step - loss: 0.6793 - acc: 0.2175\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 340us/step - loss: 0.6258 - acc: 0.2676\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 309us/step - loss: 0.5223 - acc: 0.2815\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 335us/step - loss: 0.3905 - acc: 0.2824\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 330us/step - loss: 0.2813 - acc: 0.2824\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 353us/step - loss: 0.2254 - acc: 0.2824\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 351us/step - loss: 0.2159 - acc: 0.2824\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 343us/step - loss: 0.2275 - acc: 0.2824\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 342us/step - loss: 0.2395 - acc: 0.2824\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 359us/step - loss: 0.2436 - acc: 0.2824\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 352us/step - loss: 0.2398 - acc: 0.2824\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 357us/step - loss: 0.2307 - acc: 0.2824\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 337us/step - loss: 0.2197 - acc: 0.2824\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 328us/step - loss: 0.2104 - acc: 0.2824\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 435us/step - loss: 0.2037 - acc: 0.2824\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 366us/step - loss: 0.1995 - acc: 0.2824\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 342us/step - loss: 0.1973 - acc: 0.2824\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 365us/step - loss: 0.1945 - acc: 0.2824\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 366us/step - loss: 0.1917 - acc: 0.2824\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 358us/step - loss: 0.1895 - acc: 0.2824\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 335us/step - loss: 0.1878 - acc: 0.2824\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 335us/step - loss: 0.1862 - acc: 0.2824\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 363us/step - loss: 0.1846 - acc: 0.2824\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 371us/step - loss: 0.1826 - acc: 0.2824\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 313us/step - loss: 0.1804 - acc: 0.2824\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 343us/step - loss: 0.1787 - acc: 0.2824\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 349us/step - loss: 0.1770 - acc: 0.2824\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 338us/step - loss: 0.1759 - acc: 0.2824\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 331us/step - loss: 0.1748 - acc: 0.2824\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 345us/step - loss: 0.1731 - acc: 0.2824\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 364us/step - loss: 0.1703 - acc: 0.2824\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 344us/step - loss: 0.1675 - acc: 0.2824\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 352us/step - loss: 0.1662 - acc: 0.2824\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 335us/step - loss: 0.1648 - acc: 0.2824\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 414us/step - loss: 0.1628 - acc: 0.2824\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 366us/step - loss: 0.1610 - acc: 0.2824\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 313us/step - loss: 0.1601 - acc: 0.2824\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 352us/step - loss: 0.1589 - acc: 0.2824\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 353us/step - loss: 0.1574 - acc: 0.2824\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 343us/step - loss: 0.1568 - acc: 0.2824\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 331us/step - loss: 0.1560 - acc: 0.2824\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 354us/step - loss: 0.1548 - acc: 0.2824\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 347us/step - loss: 0.1541 - acc: 0.2824\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 360us/step - loss: 0.1538 - acc: 0.2824\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 322us/step - loss: 0.1533 - acc: 0.2824\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 360us/step - loss: 0.1524 - acc: 0.2824\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 347us/step - loss: 0.1522 - acc: 0.2824\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 333us/step - loss: 0.1524 - acc: 0.2824\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 315us/step - loss: 0.1521 - acc: 0.2824\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 23s 165ms/step - loss: 0.7416 - acc: 0.2068\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 129us/step - loss: 0.4473 - acc: 0.2418\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 122us/step - loss: 0.1598 - acc: 0.2704\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 120us/step - loss: -0.1604 - acc: 0.2888\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 123us/step - loss: -0.5334 - acc: 0.3023\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 122us/step - loss: -0.9676 - acc: 0.3132\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 113us/step - loss: -1.4779 - acc: 0.3253\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 114us/step - loss: -2.0149 - acc: 0.3370\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 119us/step - loss: -2.6318 - acc: 0.3488\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 127us/step - loss: -3.3112 - acc: 0.3613\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 133us/step - loss: -4.0105 - acc: 0.3699\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 147us/step - loss: -4.7311 - acc: 0.3698\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 201us/step - loss: -5.4146 - acc: 0.3698\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 134us/step - loss: -6.0112 - acc: 0.3698\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 132us/step - loss: -6.4784 - acc: 0.3699\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 136us/step - loss: -6.8204 - acc: 0.3699\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 123us/step - loss: -7.0416 - acc: 0.3699\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 125us/step - loss: -7.1786 - acc: 0.3698\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 119us/step - loss: -7.2523 - acc: 0.3698\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 114us/step - loss: -7.2889 - acc: 0.3698\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 118us/step - loss: -7.3072 - acc: 0.3698\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 128us/step - loss: -7.3158 - acc: 0.3699\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 132us/step - loss: -7.3204 - acc: 0.3699\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 116us/step - loss: -7.3231 - acc: 0.3699\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 138us/step - loss: -7.3253 - acc: 0.3698\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 128us/step - loss: -7.3276 - acc: 0.3698\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 124us/step - loss: -7.3299 - acc: 0.3698\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 116us/step - loss: -7.3319 - acc: 0.3698\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 118us/step - loss: -7.3337 - acc: 0.3699\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 112us/step - loss: -7.3354 - acc: 0.3699\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 129us/step - loss: -7.3369 - acc: 0.3699\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 121us/step - loss: -7.3382 - acc: 0.3699\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 152us/step - loss: -7.3396 - acc: 0.3698\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 130us/step - loss: -7.3410 - acc: 0.3698\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 124us/step - loss: -7.3427 - acc: 0.3698\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 148us/step - loss: -7.3450 - acc: 0.3699\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 133us/step - loss: -7.3471 - acc: 0.3699\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 133us/step - loss: -7.3500 - acc: 0.3699\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 136us/step - loss: -7.3519 - acc: 0.3699\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 138us/step - loss: -7.3535 - acc: 0.3699\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 142us/step - loss: -7.3550 - acc: 0.3699\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 128us/step - loss: -7.3564 - acc: 0.3699\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 118us/step - loss: -7.3579 - acc: 0.3699\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 144us/step - loss: -7.3597 - acc: 0.3699\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 130us/step - loss: -7.3616 - acc: 0.3699\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 126us/step - loss: -7.3635 - acc: 0.3699\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 134us/step - loss: -7.3652 - acc: 0.3699\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 139us/step - loss: -7.3672 - acc: 0.3699\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 132us/step - loss: -7.3694 - acc: 0.3699\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 131us/step - loss: -7.3713 - acc: 0.3700\n",
            "Train on 140 samples, validate on 180 samples\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 26s 182ms/step - loss: 3.1398 - acc: 0.7310 - val_loss: 0.4669 - val_acc: 0.8333\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 346us/step - loss: 1.6416 - acc: 0.7631 - val_loss: 0.4648 - val_acc: 0.8333\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 1.6662 - acc: 0.7214 - val_loss: 0.4636 - val_acc: 0.8333\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 277us/step - loss: 1.2761 - acc: 0.7298 - val_loss: 0.4627 - val_acc: 0.8333\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 289us/step - loss: 1.1910 - acc: 0.7381 - val_loss: 0.4620 - val_acc: 0.8333\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 302us/step - loss: 1.1696 - acc: 0.7476 - val_loss: 0.4609 - val_acc: 0.8333\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 277us/step - loss: 1.0961 - acc: 0.7702 - val_loss: 0.4599 - val_acc: 0.8333\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.9531 - acc: 0.7952 - val_loss: 0.4586 - val_acc: 0.8333\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 278us/step - loss: 0.8722 - acc: 0.8048 - val_loss: 0.4572 - val_acc: 0.8333\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 287us/step - loss: 0.7453 - acc: 0.8143 - val_loss: 0.4558 - val_acc: 0.8333\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 298us/step - loss: 0.6263 - acc: 0.8262 - val_loss: 0.4547 - val_acc: 0.8333\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 299us/step - loss: 0.5491 - acc: 0.8083 - val_loss: 0.4539 - val_acc: 0.8333\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 287us/step - loss: 0.4929 - acc: 0.8238 - val_loss: 0.4533 - val_acc: 0.8333\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 371us/step - loss: 0.4473 - acc: 0.8226 - val_loss: 0.4527 - val_acc: 0.8333\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.4059 - acc: 0.8274 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.3626 - acc: 0.8417 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.3434 - acc: 0.8476 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 272us/step - loss: 0.3434 - acc: 0.8476 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 271us/step - loss: 0.3507 - acc: 0.8524 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 275us/step - loss: 0.3534 - acc: 0.8524 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 298us/step - loss: 0.3439 - acc: 0.8548 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 286us/step - loss: 0.3296 - acc: 0.8524 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 288us/step - loss: 0.3203 - acc: 0.8571 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 0.3170 - acc: 0.8583 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 273us/step - loss: 0.3137 - acc: 0.8631 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.3079 - acc: 0.8643 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 278us/step - loss: 0.3021 - acc: 0.8667 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 278us/step - loss: 0.2968 - acc: 0.8679 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 295us/step - loss: 0.2954 - acc: 0.8726 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 288us/step - loss: 0.2943 - acc: 0.8714 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 301us/step - loss: 0.2887 - acc: 0.8702 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 286us/step - loss: 0.2801 - acc: 0.8762 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 304us/step - loss: 0.2749 - acc: 0.8845 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 290us/step - loss: 0.2749 - acc: 0.8786 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.2725 - acc: 0.8798 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 321us/step - loss: 0.2677 - acc: 0.8881 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 290us/step - loss: 0.2614 - acc: 0.9024 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 322us/step - loss: 0.2564 - acc: 0.9024 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 358us/step - loss: 0.2555 - acc: 0.9060 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 301us/step - loss: 0.2557 - acc: 0.8988 - val_loss: 0.4526 - val_acc: 0.8333\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 307us/step - loss: 0.2541 - acc: 0.8964 - val_loss: 0.4526 - val_acc: 0.8333\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 306us/step - loss: 0.2480 - acc: 0.8964 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 308us/step - loss: 0.2391 - acc: 0.8964 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 306us/step - loss: 0.2382 - acc: 0.8976 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 0.2436 - acc: 0.9048 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.2426 - acc: 0.9024 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 299us/step - loss: 0.2307 - acc: 0.9119 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.2227 - acc: 0.9167 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 275us/step - loss: 0.2214 - acc: 0.9071 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 334us/step - loss: 0.2180 - acc: 0.9119 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "(140, 6912)\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 24s 170ms/step - loss: 0.6929 - acc: 0.1343\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 274us/step - loss: 0.6763 - acc: 0.2084\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 305us/step - loss: 0.6123 - acc: 0.2537\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 261us/step - loss: 0.4941 - acc: 0.2636\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 264us/step - loss: 0.3566 - acc: 0.2646\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 252us/step - loss: 0.2570 - acc: 0.2646\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 268us/step - loss: 0.2151 - acc: 0.2646\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 0.2178 - acc: 0.2646\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 264us/step - loss: 0.2285 - acc: 0.2646\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 289us/step - loss: 0.2364 - acc: 0.2646\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 274us/step - loss: 0.2358 - acc: 0.2646\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 278us/step - loss: 0.2283 - acc: 0.2646\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 269us/step - loss: 0.2184 - acc: 0.2646\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 281us/step - loss: 0.2093 - acc: 0.2646\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 278us/step - loss: 0.2037 - acc: 0.2646\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 251us/step - loss: 0.2004 - acc: 0.2646\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 262us/step - loss: 0.1981 - acc: 0.2646\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 0.1954 - acc: 0.2646\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 264us/step - loss: 0.1928 - acc: 0.2646\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.1908 - acc: 0.2646\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 286us/step - loss: 0.1893 - acc: 0.2646\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 269us/step - loss: 0.1881 - acc: 0.2646\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 268us/step - loss: 0.1868 - acc: 0.2646\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 0.1854 - acc: 0.2646\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 295us/step - loss: 0.1840 - acc: 0.2646\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.1825 - acc: 0.2646\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 274us/step - loss: 0.1811 - acc: 0.2646\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.1800 - acc: 0.2646\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 314us/step - loss: 0.1787 - acc: 0.2646\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 258us/step - loss: 0.1771 - acc: 0.2646\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 266us/step - loss: 0.1755 - acc: 0.2646\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 303us/step - loss: 0.1742 - acc: 0.2646\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 284us/step - loss: 0.1731 - acc: 0.2646\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 265us/step - loss: 0.1724 - acc: 0.2646\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 278us/step - loss: 0.1713 - acc: 0.2646\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 256us/step - loss: 0.1697 - acc: 0.2646\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 257us/step - loss: 0.1684 - acc: 0.2646\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 254us/step - loss: 0.1672 - acc: 0.2646\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 253us/step - loss: 0.1661 - acc: 0.2646\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 251us/step - loss: 0.1651 - acc: 0.2646\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 249us/step - loss: 0.1647 - acc: 0.2646\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 253us/step - loss: 0.1640 - acc: 0.2646\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 271us/step - loss: 0.1629 - acc: 0.2646\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 271us/step - loss: 0.1621 - acc: 0.2646\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 305us/step - loss: 0.1615 - acc: 0.2646\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 269us/step - loss: 0.1607 - acc: 0.2646\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 246us/step - loss: 0.1600 - acc: 0.2646\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 267us/step - loss: 0.1594 - acc: 0.2646\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.1590 - acc: 0.2646\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 268us/step - loss: 0.1589 - acc: 0.2646\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 24s 172ms/step - loss: 0.8091 - acc: 0.2152\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 168us/step - loss: 0.4351 - acc: 0.2444\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 162us/step - loss: 0.0825 - acc: 0.2744\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 160us/step - loss: -0.2949 - acc: 0.2946\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 155us/step - loss: -0.7174 - acc: 0.3299\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 140us/step - loss: -1.2158 - acc: 0.3432\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 139us/step - loss: -1.7663 - acc: 0.3549\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 143us/step - loss: -2.4079 - acc: 0.3687\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 155us/step - loss: -3.1039 - acc: 0.3746\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 147us/step - loss: -3.8996 - acc: 0.3807\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 144us/step - loss: -4.7159 - acc: 0.3844\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 166us/step - loss: -5.5482 - acc: 0.3844\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 163us/step - loss: -6.3081 - acc: 0.3844\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 180us/step - loss: -6.9158 - acc: 0.3862\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 157us/step - loss: -7.3459 - acc: 0.3883\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 147us/step - loss: -7.6178 - acc: 0.3883\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 191us/step - loss: -7.7754 - acc: 0.3884\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 169us/step - loss: -7.8631 - acc: 0.3883\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 177us/step - loss: -7.9118 - acc: 0.3883\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 195us/step - loss: -7.9397 - acc: 0.3883\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 220us/step - loss: -7.9572 - acc: 0.3883\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 193us/step - loss: -7.9662 - acc: 0.3883\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 187us/step - loss: -7.9712 - acc: 0.3883\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 196us/step - loss: -7.9737 - acc: 0.3884\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 168us/step - loss: -7.9744 - acc: 0.3883\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 184us/step - loss: -7.9748 - acc: 0.3883\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 195us/step - loss: -7.9751 - acc: 0.3883\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 196us/step - loss: -7.9752 - acc: 0.3883\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 204us/step - loss: -7.9754 - acc: 0.3883\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 165us/step - loss: -7.9758 - acc: 0.3883\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 165us/step - loss: -7.9763 - acc: 0.3883\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 150us/step - loss: -7.9769 - acc: 0.3883\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 156us/step - loss: -7.9774 - acc: 0.3883\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 162us/step - loss: -7.9772 - acc: 0.3884\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 185us/step - loss: -7.9778 - acc: 0.3884\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 173us/step - loss: -7.9791 - acc: 0.3883\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 157us/step - loss: -7.9794 - acc: 0.3883\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 149us/step - loss: -7.9795 - acc: 0.3883\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 150us/step - loss: -7.9801 - acc: 0.3883\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 217us/step - loss: -7.9809 - acc: 0.3883\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 255us/step - loss: -7.9817 - acc: 0.3884\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 162us/step - loss: -7.9823 - acc: 0.3885\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 165us/step - loss: -7.9832 - acc: 0.3884\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 170us/step - loss: -7.9842 - acc: 0.3883\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 173us/step - loss: -7.9852 - acc: 0.3883\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 177us/step - loss: -7.9857 - acc: 0.3883\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 189us/step - loss: -7.9868 - acc: 0.3883\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 223us/step - loss: -7.9872 - acc: 0.3883\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 171us/step - loss: -7.9874 - acc: 0.3883\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 168us/step - loss: -7.9878 - acc: 0.3884\n",
            "Train on 140 samples, validate on 180 samples\n",
            "Epoch 1/50\n",
            "140/140 [==============================] - 26s 186ms/step - loss: 3.2422 - acc: 0.7143 - val_loss: 0.4590 - val_acc: 0.8333\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 0s 340us/step - loss: 1.5377 - acc: 0.7214 - val_loss: 0.4586 - val_acc: 0.8333\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 0s 302us/step - loss: 1.6501 - acc: 0.7238 - val_loss: 0.4582 - val_acc: 0.8333\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 0s 379us/step - loss: 1.3929 - acc: 0.7738 - val_loss: 0.4579 - val_acc: 0.8333\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 0s 304us/step - loss: 1.4254 - acc: 0.7536 - val_loss: 0.4573 - val_acc: 0.8333\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 0s 298us/step - loss: 1.4110 - acc: 0.7548 - val_loss: 0.4562 - val_acc: 0.8333\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 0s 298us/step - loss: 1.2555 - acc: 0.7821 - val_loss: 0.4551 - val_acc: 0.8333\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 1.0169 - acc: 0.7643 - val_loss: 0.4539 - val_acc: 0.8333\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 0s 277us/step - loss: 0.7093 - acc: 0.8357 - val_loss: 0.4529 - val_acc: 0.8333\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 0s 279us/step - loss: 0.5399 - acc: 0.8286 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 0s 304us/step - loss: 0.5176 - acc: 0.8119 - val_loss: 0.4517 - val_acc: 0.8333\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.4665 - acc: 0.8012 - val_loss: 0.4514 - val_acc: 0.8333\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 0s 301us/step - loss: 0.4175 - acc: 0.8286 - val_loss: 0.4513 - val_acc: 0.8333\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 0s 277us/step - loss: 0.4330 - acc: 0.8131 - val_loss: 0.4514 - val_acc: 0.8333\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 0s 277us/step - loss: 0.4045 - acc: 0.8167 - val_loss: 0.4517 - val_acc: 0.8333\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.3688 - acc: 0.8321 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 0s 282us/step - loss: 0.3505 - acc: 0.8369 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 0.3439 - acc: 0.8488 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 0s 288us/step - loss: 0.3393 - acc: 0.8500 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.3339 - acc: 0.8512 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 0s 324us/step - loss: 0.3235 - acc: 0.8536 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 0s 302us/step - loss: 0.3105 - acc: 0.8536 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.3000 - acc: 0.8631 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.2931 - acc: 0.8750 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 0s 280us/step - loss: 0.2868 - acc: 0.8762 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 0s 308us/step - loss: 0.2821 - acc: 0.8750 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 0s 310us/step - loss: 0.2792 - acc: 0.8798 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 0s 357us/step - loss: 0.2772 - acc: 0.8857 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 0s 297us/step - loss: 0.2788 - acc: 0.8774 - val_loss: 0.4520 - val_acc: 0.8333\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 0s 272us/step - loss: 0.2754 - acc: 0.8738 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 0s 290us/step - loss: 0.2648 - acc: 0.8810 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.2618 - acc: 0.8845 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 0s 283us/step - loss: 0.2624 - acc: 0.8810 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 0s 285us/step - loss: 0.2560 - acc: 0.8833 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 0s 299us/step - loss: 0.2464 - acc: 0.8964 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 0s 299us/step - loss: 0.2436 - acc: 0.8988 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 0s 284us/step - loss: 0.2435 - acc: 0.9071 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 0s 291us/step - loss: 0.2432 - acc: 0.9071 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 0s 298us/step - loss: 0.2410 - acc: 0.9095 - val_loss: 0.4522 - val_acc: 0.8333\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 0s 311us/step - loss: 0.2343 - acc: 0.9095 - val_loss: 0.4523 - val_acc: 0.8333\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 0s 288us/step - loss: 0.2269 - acc: 0.9095 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 0s 287us/step - loss: 0.2191 - acc: 0.9095 - val_loss: 0.4524 - val_acc: 0.8333\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 0s 292us/step - loss: 0.2211 - acc: 0.9036 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 0s 296us/step - loss: 0.2247 - acc: 0.9024 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 0s 304us/step - loss: 0.2135 - acc: 0.9107 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 0s 302us/step - loss: 0.2098 - acc: 0.9179 - val_loss: 0.4526 - val_acc: 0.8333\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 0s 295us/step - loss: 0.2185 - acc: 0.9190 - val_loss: 0.4526 - val_acc: 0.8333\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 0s 299us/step - loss: 0.2168 - acc: 0.9155 - val_loss: 0.4525 - val_acc: 0.8333\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 0s 302us/step - loss: 0.2084 - acc: 0.9179 - val_loss: 0.4526 - val_acc: 0.8333\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 0s 329us/step - loss: 0.2031 - acc: 0.9143 - val_loss: 0.4526 - val_acc: 0.8333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3d04a2f2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k8iDGRRZ1q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probscore1=final1.predict_proba(X_test)\n",
        "probscore2=final2.predict_proba(X_test)\n",
        "probscore3=final3.predict_proba(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv7qRkuxcGqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = 0\n",
        "ytrue=[]\n",
        "ypred=[]\n",
        "for i in range(len(probscore1)):\n",
        "    probscore = [] \n",
        "    probscore.append(probscore1[i])\n",
        "    probscore.append(probscore2[i])\n",
        "    probscore.append(probscore3[i])\n",
        "    overall = probscore[0]+probscore[1]+probscore[2]\n",
        "    \n",
        "    final_probscore = np.argmax(overall)\n",
        "    ytrue.append(np.argmax(Y_test[i]))\n",
        "    ypred.append(final_probscore)\n",
        "    if Y_test[i][final_probscore]==1:\n",
        "      c=c+1\n",
        "\n",
        "print((c/len(probscore1))*100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}